{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3OIsqxJTpB8"
      },
      "source": [
        "# **MULTI UAV CONFLICT RISK ANALYSIS - CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Cx9ib4oqmB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **IMPORT**\n",
        "Importing the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT2C67ChyGmq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# data processing \n",
        "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score, accuracy_score, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ePg5RLmPWr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **MOUNT DRIVE**\n",
        "Mounting Google Drive to then load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wCSWcO2zB2H"
      },
      "outputs": [],
      "source": [
        "# mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HlyDqhmnC-G"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **LOAD THE DATASET**\n",
        "In order to load the dataset, i first set its current location in my drive (to avoid errors, check your path and replace it). Then, since this is a tabular-separated values file, i read it using *panda.read_csv()* which loads it into a DataFrame. \n",
        "\n",
        "It is possible to modify the query to work with only some samples and to test different configurations. Then, i make the split between input and output columns. The input are stored in the first 35 columns, while the outputs for classification are stored in the 36th column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PjCbTREBPVi"
      },
      "source": [
        "## Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4WMV6XSzUmb"
      },
      "outputs": [],
      "source": [
        "# importing the file from drive and reading it into DataFrame\n",
        "filename = '/content/drive/MyDrive/Project_ML/Data/train_set.tsv'\n",
        "dataframe = pd.read_csv(filename, sep = \"\\t\", header = 0)\n",
        "print('File loaded: %d samples.' %(len(dataframe)))\n",
        "\n",
        "# the query can be changed to work with only some samples belonging to the specified class\n",
        "query = '(num_collisions ==0) or (num_collisions==1) or (num_collisions==2) or (num_collisions==3) or (num_collisions ==4)'\n",
        "df = dataframe.query(query)\n",
        "\n",
        "# dividing input and output columns\n",
        "X = df.iloc[:, :-2]\n",
        "y = df.iloc[:, -2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnxMia89xtds"
      },
      "source": [
        "## Correlations and missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI8vbzbD1di7"
      },
      "source": [
        "It is important to know if there are some missing values in the dataset and eventually replace them. In our case, there aren't missing values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaB8lBgp1VrB"
      },
      "outputs": [],
      "source": [
        "print(\"Number of null cells in df: %d\" %(df.isnull().sum().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsrOCuLCWant"
      },
      "source": [
        "I can even display correlations between the features in the dataset. From there you can see that there are almost no correlations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PyWIg-5V4LJ"
      },
      "outputs": [],
      "source": [
        "X.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEM7y_GuzfXS"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **DATA VISUALIZATION**\n",
        "Before starting processing the dataset it's important to visualize it to have an overview of our data and to decide how to proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QllHOA7c3epW"
      },
      "source": [
        "## Histogram\n",
        "This first histogram shows how the samples are distributed. As you can see, the dataset is very imbalanced. More than 500 samples belong to the first class, while the last class has only three samples.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmSAd0m30HBT"
      },
      "outputs": [],
      "source": [
        "# Histogram plot of the class distribution\n",
        "distr_plot = sns.histplot(\n",
        "    data = df, \n",
        "    x='num_collisions', \n",
        "    hue = 'num_collisions',\n",
        "    discrete = True,  \n",
        "    shrink=0.8)\n",
        "Counter(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VShdRcse496m"
      },
      "source": [
        "## Pie Plot\n",
        "This imbalance can be visualized better using a pie chart and plotting the percentage of examples for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-slaPWtzj0_"
      },
      "outputs": [],
      "source": [
        "# Pie plot of the class distribution\n",
        "colors = sns.color_palette('pastel')[0:5]\n",
        "\n",
        "print(\"CLASS DISTRIBUTION:\")\n",
        "plot = df['num_collisions'].value_counts().plot.pie(\n",
        "    autopct='%.2f',\n",
        "    colors = colors, \n",
        "    figsize=(8, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7mc93_DH7Ts"
      },
      "source": [
        "## Two features scatter plot\n",
        "In this section it's possible to plot some features to see how they are related to each other and with the belonging class. \n",
        "\n",
        "*Obviosly this is just a try and a preliminary analysis.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2zxnr847eE9"
      },
      "outputs": [],
      "source": [
        "def two_features_plot(x, feature1, feature2):\n",
        "  fig, ax = plt.subplots(figsize=(8,8))\n",
        "  scatter_plot= sns.scatterplot(\n",
        "      data=df,\n",
        "      x=x.iloc[:,feature1], y=x.iloc[:,feature2],\n",
        "      hue=\"num_collisions\",\n",
        "      size = \"num_collisions\",\n",
        "      ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaZOBF-v_gwI"
      },
      "outputs": [],
      "source": [
        "# features from 0 to 34\n",
        "two_features_plot(X,1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NCylB698ss"
      },
      "source": [
        "## One feature density plot\n",
        "In this section it's possible to plot one feature to see if there are single features that are more relevant to predict the class. \n",
        "\n",
        "*Obviosly this is just a try and a preliminary analysis.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W96gkzieG6e7"
      },
      "outputs": [],
      "source": [
        "def feature_plot(x, feature):\n",
        "  fig, ax = plt.subplots(figsize=(8,8))\n",
        "  kde_plot = sns.kdeplot(data=df, x = X.iloc[:, feature], hue = 'num_collisions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKVrNUGOGxfM"
      },
      "outputs": [],
      "source": [
        "# features from 0 to 34\n",
        "feature_plot(X, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckONiJFNawcA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **FEATURE SCALING**\n",
        "\n",
        "Here, there's the possibility to scale the features column-wise with four different methods or to not scale at all.\n",
        "\n",
        "The first method is the **Maximum Absolute Scaling** which returns values of the input data between -1 and 1. It takes the input and it divides it by the maximum absolute value on that column.\n",
        "\n",
        "The second method is the **Min-Max Feature Scaling**, also called normalization, which scales the feature between 0 and 1. It's computed by subtracting from the input the minimum value in the column and subsequently dividing by the difference between the maximum and minimum value.\n",
        "\n",
        "The third method is the **Standard Scaler** and scales the data into a distribution with zero mean and variance 1.\n",
        "\n",
        "The last method is the **Robust Scaling** which removes the median and scales the data according to the quantile range.\n",
        "\n",
        "I will use StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbyhQ98iEROb"
      },
      "outputs": [],
      "source": [
        "# \"MAS\": Maximum Absolute Scaling, \"MMS\": Min-Max Scaling or \"SS\": Standard Scaler, \"RS\": Robust Scaling\n",
        "s = \"SS\"\n",
        "def scaling(series, scaling_type):\n",
        "  if scaling_type == \"MAS\":\n",
        "    scaler = MaxAbsScaler()\n",
        "  elif scaling_type == \"MMS\":\n",
        "    scaler = MinMaxScaler()\n",
        "  elif scaling_type == \"SS\":\n",
        "    scaler = StandardScaler()\n",
        "  elif scaling_type == \"RS\":\n",
        "    scaler = RobustScaler()\n",
        "  else:\n",
        "    return series\n",
        "  scaler.fit(series)\n",
        "  scaled = scaler.fit_transform(series)\n",
        "  series = pd.DataFrame(scaled, columns = series.columns)\n",
        "  return series\n",
        "\n",
        "X = scaling(X, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcpm0oknEzBS"
      },
      "source": [
        "We can see the effect of the scaling on two desired features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3jaf2PWE_2T"
      },
      "outputs": [],
      "source": [
        "two_features_plot(X, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbOVBIQgwwE4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **DATA SPLITTING AND RESAMPLING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4RS7nrA4BJi"
      },
      "source": [
        "## Splitting the dataset\n",
        "I've splitted the dataset in training and test set.The model must be trained with the training data and then tested. Comparing predictions to true labels in the test set can be seen as the unbiased performance evaluation of the model. Since i have an imbalanced dataset, providing the class label array y as an argument to stratify ensures that both training and test datasets have the same class proportions as the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBAPyapy3IKc"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "          test_size=0.3, random_state=24, stratify = y)\n",
        "\n",
        "# This below is just a check on the classes count in the test set. \n",
        "y_test.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OIdDdv04cmo"
      },
      "source": [
        "## Resampling\n",
        "Here, you can resample the training set. A resample method repeatedly draw samples from the dataset. Oversample can be useful in this case since the dataset is severely imbalanced. I've decided to try three different techniques of oversampling:\n",
        "\n",
        "**SMOTE**: or Synthetic Minority Oversampling TEchnique works by selecting examples that are close in the feature space , drawing a line between them and drawing a new sample at a point along that line.\n",
        "\n",
        "**ADASYN**: this method is similar to SMOTE but it generates different number of samples depending on an estimate of the local distribution of the class to be oversampled.\n",
        "\n",
        "**RandomOverSampler**: it's the most naive strategy to generate new samples by randomly sampling with replacement of the current available samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZUtvtFf6tcf"
      },
      "outputs": [],
      "source": [
        "# \"SMOTE\", \"ADASYN\", \"ROV\": RandomOverSampler, \n",
        "res = \"ADASYN\"\n",
        "\n",
        "def resample(X, y, res):\n",
        "  if res == \"SMOTE\":\n",
        "    oversample = SMOTE(k_neighbors = 1)\n",
        "  elif res == \"ADASYN\":\n",
        "    oversample = ADASYN(n_neighbors = 1)\n",
        "  elif res == \"ROV\":\n",
        "    oversample = RandomOverSampler(sampling_strategy=\"all\")  \n",
        "  \n",
        "  X_res, y_res = oversample.fit_resample(X, y)\n",
        "  print('Resampled dataset shape %s' % Counter(y_res))\n",
        "\n",
        "  return X_res, y_res\n",
        "\n",
        "X_res, y_res = resample(X_train, y_train, res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDHCmITk6-ue"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **MODEL DEFINITION**\n",
        "Here, i've defined a bunch of models to evaluate and then to make a comparison. I've decided to compare **SVC**, **DecisionTreeClassifier**, and **RandomForestClassifier**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpDdnLqHYGGb"
      },
      "outputs": [],
      "source": [
        "# models\n",
        "rand_forest = RandomForestClassifier(class_weight='balanced')\n",
        "svc = SVC()\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# list of models to evaluate\n",
        "models = [rand_forest, svc, dt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLgqfZ1De1Yy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **EVALUATION OF NOT OPTIMIZED MODELS**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxrubMQmfZCP"
      },
      "source": [
        "In this section, i analyzed the main metrics for each model (from the previous section) trained with the training set. I've decided to use f1_macro, f1_weighted and balanced accuracy. Thus, i've created a function that does exactly this, it displays the performance of the models on train and test set in a compact way. Then, i've displayed the confusion matrices. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6-eHE_0ClEC"
      },
      "outputs": [],
      "source": [
        "def models_scores(models, X_train, y_train, X_test, y_test): \n",
        "  # train metrics lists\n",
        "  f1_macro_train_list = []\n",
        "  f1_weighted_train_list = []\n",
        "  balanced_accuracy_train_list = []\n",
        "  accuracy_train_list=[]\n",
        "\n",
        "  #test metrics lists \n",
        "  f1_macro_test_list = []\n",
        "  f1_weighted_test_list = []\n",
        "  balanced_accuracy_test_list = []\n",
        "  accuracy_test_list=[]\n",
        "\n",
        "  names = []\n",
        "\n",
        "  for model in models:\n",
        "      #append the name of the model to the names list\n",
        "      names.append(type(model).__name__)\n",
        "\n",
        "      # fit the model and predict\n",
        "      model.fit(X_train,y_train)\n",
        "      y_pred_train = model.predict(X_train)\n",
        "      y_pred_test = model.predict(X_test)\n",
        "      \n",
        "\n",
        "      # compute the metrics for training set\n",
        "      f1_macro_train = f1_score(y_train, y_pred_train, average = 'macro')\n",
        "      f1_weighted_train = f1_score(y_train, y_pred_train, average = 'weighted')\n",
        "      balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "      accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "      # compute the metrics for test set\n",
        "      f1_macro_test = f1_score(y_test, y_pred_test, average ='macro')\n",
        "      f1_weighted_test = f1_score(y_test, y_pred_test, average = 'weighted')\n",
        "      balanced_accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "      accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "      # add train metrics to the lists\n",
        "      f1_macro_train_list.append(f1_macro_train)\n",
        "      f1_weighted_train_list.append(f1_weighted_train)\n",
        "      balanced_accuracy_train_list.append(balanced_accuracy_train)\n",
        "      accuracy_train_list.append(accuracy_train)\n",
        "\n",
        "      # add test metrics to the lists\n",
        "      f1_macro_test_list.append(f1_macro_test)\n",
        "      f1_weighted_test_list.append(f1_weighted_test)\n",
        "      balanced_accuracy_test_list.append(balanced_accuracy_test)\n",
        "      accuracy_test_list.append(accuracy_test)\n",
        "  \n",
        "  d = {\n",
        "      'Model': names, \n",
        "      'balanced_accuracy_train':  balanced_accuracy_train_list, \n",
        "      'balanced_accuracy_test': balanced_accuracy_test_list,\n",
        "      'F1_macro_train': f1_macro_train_list,\n",
        "      'F1_macro_test':  f1_macro_test_list, \n",
        "      'F1_weighted_train': f1_weighted_train_list, \n",
        "      'F1_weighted_test': f1_weighted_test_list, \n",
        "      'accuracy_train': accuracy_train_list, \n",
        "      'accuracy_test': accuracy_test_list}\n",
        "      \n",
        "  scores = pd.DataFrame(d)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKGrfW-phhhc"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQdoKygeGouC"
      },
      "outputs": [],
      "source": [
        "# metrics of models trained with the original training set\n",
        "models_scores(models, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRFrzNtRvyDk"
      },
      "outputs": [],
      "source": [
        "# metrics of models trained with the resampled training set\n",
        "models_scores(models, X_res, y_res, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHWk6cXqhkel"
      },
      "source": [
        "## Confusion Matrices\n",
        "A confusion matrix is simply a square matrix that reports the counts of the true positive, true negative, false positive, false negative predictions of a classifier. Here below, the confusion matrices of the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUh_wVo7c-A_"
      },
      "outputs": [],
      "source": [
        "def display_confusion_matrices(models, X, y):\n",
        "  if len(models)>1:\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(models), figsize=(35,10))\n",
        "    for model, ax in zip(models, axes.flatten()):\n",
        "      ax.set_title(type(model).__name__)\n",
        "      ax.grid(False)\n",
        "      display = ConfusionMatrixDisplay.from_estimator(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize='true',\n",
        "        ax = ax)\n",
        "  else:\n",
        "    display = ConfusionMatrixDisplay.from_estimator(\n",
        "    models[0],\n",
        "    X,\n",
        "    y,\n",
        "    cmap=plt.cm.Blues,\n",
        "    normalize='true')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg-hpRMvdAIQ"
      },
      "outputs": [],
      "source": [
        "display_confusion_matrices(models, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxYr6Dtulah_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **HYPERPARAMETER TUNING OF SVC, DECISION TREE, RANDOM FOREST**\n",
        "Since the performances were bad, i've analyzed these classifiers to see if they can be improved using GridSearch. I used as scoring balanced accuracy, which i thought to be one of the most representative performance metrics for this task. In fact, using accuracy would have probably led to a set of hyperparameters that classify all the samples as belonging to the zero class, the most populated one. \n",
        "\n",
        "Note: the grid search may take some time, you can decide not to run it. The best parameters found for each model are already saved below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMNSuPQNE-RK"
      },
      "source": [
        "## Grid search\n",
        "\n",
        "Choose the model from which you want to perform gridsearch, given multiple possible values. These values were obtained after reading the documentation of the classifiers and after several trials. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVz-Iz1RdW-d"
      },
      "outputs": [],
      "source": [
        "# Choose your classifier: \"SVC\": Support Vector Machine Classifier, \"DT\": Decition Tree Classifier, \"RFC\": Random Forest Classifier\n",
        "clf = 'SVC'\n",
        "def define_grid(classifier):\n",
        "  if classifier == 'SVC':\n",
        "    estimator  = SVC()\n",
        "    param_grid = {\n",
        "      'C': np.arange(0.1, 1, 0.1), \n",
        "      'gamma': [1, 0.1, 0.001, 'scale', 'auto'],\n",
        "      'degree': [2, 3, 4],\n",
        "      'kernel': ['rbf', 'poly', 'sigmoid'],\n",
        "      'class_weight': ['balanced', None],\n",
        "      }\n",
        "  elif classifier == 'DT':\n",
        "    estimator = DecisionTreeClassifier()\n",
        "    param_grid ={\n",
        "      'max_depth':np.arange(2, 10, 1),\n",
        "      'class_weight':['balanced', None],\n",
        "      'splitter': ['best', 'random']\n",
        "  }\n",
        "  elif classifier == 'RFC':\n",
        "    estimator = RandomForestClassifier(n_jobs = -1)\n",
        "    param_grid ={\n",
        "        'n_estimators':[2, 4, 6, 10, 15, 20, 30, 50, 60, 90, 100, 150],\n",
        "        'max_depth':[2,3,4,5, 6,7],\n",
        "        'class_weight':['balanced', None],\n",
        "        'max_features':['sqrt', 'log2', None],\n",
        "  }\n",
        "  return estimator, param_grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45u2DewPaBgP"
      },
      "outputs": [],
      "source": [
        "estimator, param_grid = define_grid(clf)\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=3, n_jobs =-1, scoring='balanced_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuGPO1HAi3d"
      },
      "source": [
        "If you want to avoid GridSearch skip this part, the best combinations of parameters have been already found and saved. You can load and evaluate the models in the next section "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCqE7aAqudWq"
      },
      "outputs": [],
      "source": [
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTGtEFMa_nQf"
      },
      "source": [
        "If you'd performed GridSearch, you can visualize the best parameters found for the chosen model and the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDNXN8Nt35My"
      },
      "outputs": [],
      "source": [
        "#Best parameters for the classifier\n",
        "print(\"Best classification hyper-parameters for the chosen classifier: %r\" %grid_search.best_params_)\n",
        "print(\"Best balanced accuracy: %.4f\" %grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODnyOShGF5_J"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **EVALUATION AFTER HYPERPARAMETER OPTIMIZATION**\n",
        "After the Grid search it is necessary to train the new models on the train set and to evaluate it on both the test and train set. This is done in order to visualize the performance and to deduce if the models overfit the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2SPEZ_2bBrL"
      },
      "source": [
        "## F1 Scores and Accuracies\n",
        "If you haven't runned the Grid Search, which is quite long, use the first lines which have the best hyperparameters already set.\n",
        "In the first cell, i've evaluated the models trained on the original training set, while in the second cell, i've evaluated the models trained on the resampled training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JSlzGujfBOX"
      },
      "outputs": [],
      "source": [
        "# best parameters found for the models with GridSearch, evaluation of models trained with original training set\n",
        "# you can even extract the best model found by the GridSearch using grid_search.best_estimator_\n",
        "svc = SVC(C=0.7, class_weight ='balanced', degree = 2, kernel = 'poly', gamma = 'scale')\n",
        "decision_tree = DecisionTreeClassifier(max_depth = 5, class_weight = 'balanced', splitter = 'random', random_state=2)\n",
        "random_forest = RandomForestClassifier(n_estimators = 50, class_weight='balanced',  max_depth = 2, random_state = 4)\n",
        "\n",
        "\n",
        "\n",
        "best_models = [svc, decision_tree, random_forest]\n",
        "df_metrics = models_scores(best_models, X_train, y_train, X_test, y_test)\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUdkXj93C12C"
      },
      "outputs": [],
      "source": [
        "# best parameters found for the models with GridSearch, evaluation of models trained with resampled dataset\n",
        "svc_res = SVC(C=0.0094, class_weight = 'balanced', degree = 2, kernel = 'poly', gamma = 0.2)\n",
        "decision_tree_res = DecisionTreeClassifier(max_depth = 10, splitter = 'best', random_state=4)\n",
        "random_forest_res = RandomForestClassifier(n_estimators = 12, max_depth = 4, random_state =4)\n",
        "\n",
        "\n",
        "best_models_res = [svc_res, decision_tree_res, random_forest_res]\n",
        "df_metrics_res = models_scores(best_models_res, X_res, y_res, X_test, y_test)\n",
        "df_metrics_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBX1FmXAbJU7"
      },
      "source": [
        "## Confusion matrices\n",
        "It is also useful to display the confusion matrices both on the test and train set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYVZa6lqmYCD"
      },
      "source": [
        "### Test set Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYjKF5K3GzK5"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the test set with clf trained on original training set\")\n",
        "display_confusion_matrices(best_models, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShpNWcQ6DMib"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the test set with clf trained on resampled training set\")\n",
        "display_confusion_matrices(best_models_res, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8jlh0BEmeMM"
      },
      "source": [
        "### Train set Confusion Matrices\n",
        "Useful to visualize overtfitting.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQBDMyN5mDRW"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on original training set with clf trained on original training set\")\n",
        "display_confusion_matrices(best_models, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLI7o7JamDcC"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the resampled training set with clf trained on resampled training set\")\n",
        "display_confusion_matrices(best_models_res, X_res, y_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw097b07Bzdf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **BAGGING**\n",
        "In this section, i've tried bagging. Bagging is an ensemble learning technique where each classifier receives a random subset of examples from the training dataset. Once the individual classifier are fit to the bootstrap samples, the predictions are combined together.\n",
        "I've used BaggingClassifier from the scikit-learn library with svc with polynomial kernel as base estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNIDc1ntByEE"
      },
      "outputs": [],
      "source": [
        "base_estimator = SVC(C=0.85, class_weight ='balanced', degree = 2, kernel = 'poly', gamma = 'scale')\n",
        "base_estimator_res = SVC(C=0.0094, class_weight = 'balanced', degree = 2, kernel = 'poly', gamma = 0.2)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator = base_estimator,\n",
        "    n_estimators = 100,\n",
        "    random_state=0\n",
        "    )\n",
        "\n",
        "bagging_res = BaggingClassifier(\n",
        "    base_estimator = base_estimator_res,\n",
        "    n_estimators = 50,\n",
        "    random_state = 0\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8QmJqk8L-jo"
      },
      "outputs": [],
      "source": [
        "model = [bagging]\n",
        "model_res = [bagging_res]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLDYTXFOLMOX"
      },
      "source": [
        "## Metrics\n",
        "Here, the main metrics are reported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ZdJuWnJX2R"
      },
      "outputs": [],
      "source": [
        "# metrics for the model trained with the original dataset\n",
        "models_scores(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7zaWKkrKVjr"
      },
      "outputs": [],
      "source": [
        "# metrics for the model trained with the resampled dataset \n",
        "models_scores(model_res, X_res, y_res, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n",
        "\n",
        "It is useful to display the confusion matrices both on the test and train set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGUUgVc4MEvu"
      },
      "source": [
        "### Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdkzU0ZVJqLm"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the original test set with clf trained on original training set\")\n",
        "display_confusion_matrices(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWFKmQQXJwWu"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the original test set with clf trained on resampled training set\")\n",
        "display_confusion_matrices(model_res, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkwQ3JCPMIcm"
      },
      "source": [
        "### Training set\n",
        "\n",
        "The following matrices are useful to visualize overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzjvhdbsMigX"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the original training set with clf trained on original training set\")\n",
        "display_confusion_matrices(model, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZGH8VuMhmj"
      },
      "outputs": [],
      "source": [
        "print(\"confusion matrices of predictions on the resampled training set with clf trained on resampled training set\")\n",
        "display_confusion_matrices(model_res, X_res, y_res)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B9Cx9ib4oqmB",
        "J2ePg5RLmPWr",
        "3HlyDqhmnC-G",
        "WEM7y_GuzfXS",
        "ckONiJFNawcA",
        "NbOVBIQgwwE4",
        "TDHCmITk6-ue",
        "SLgqfZ1De1Yy",
        "lxYr6Dtulah_",
        "ODnyOShGF5_J",
        "uw097b07Bzdf",
        "GLDYTXFOLMOX",
        "DOLyR0HELP3m"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
