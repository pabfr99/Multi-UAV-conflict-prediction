{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B9Cx9ib4oqmB",
        "J2ePg5RLmPWr",
        "3HlyDqhmnC-G",
        "7yMXQcErjtjl",
        "NbOVBIQgwwE4",
        "TDHCmITk6-ue",
        "Rjs2wYuabJcu",
        "SLgqfZ1De1Yy",
        "lxYr6Dtulah_",
        "aMNSuPQNE-RK",
        "iU2D28Gm98Vl",
        "Gmw4iyIeeWxV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MULTI UAV CONFLICT RISK ANALYSIS - REGRESSION**"
      ],
      "metadata": {
        "id": "j3OIsqxJTpB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**IMPORT**\n",
        "Import the required packages"
      ],
      "metadata": {
        "id": "B9Cx9ib4oqmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# data processing \n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "nT2C67ChyGmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**MOUNT DRIVE**\n",
        "Mount Google Drive to then load the dataset"
      ],
      "metadata": {
        "id": "J2ePg5RLmPWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_wCSWcO2zB2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**LOAD THE DATASET AND FEATURE SCALING**\n",
        "In order to load the dataset, i first set its current location in my drive (to avoid errors, check your path and replace it). Then, since this is a tabular-separated values file, i read it using *panda.read_csv()* which loads it into a DataFrame. \n",
        "\n",
        "It is possible to change *feature_number* to plot some relations between features and min_CPA and to test different configurations. There's also a function to use the whole dataset. It splits between input and output columns. The input are stored in the first 35 columns of df, while the outputs for regression are stored in the last column."
      ],
      "metadata": {
        "id": "3HlyDqhmnC-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "While extracting the dataset, i've also performed feature scaling.\n",
        "Here, there's the possibility to scale the features column-wise with four different methods or to not scale at all.\n",
        "\n",
        "The first method is the **Maximum Absolute Scaling** which returns values of the input data between -1 and 1. It takes the input and it divides it by the maximum absolute value on that column.\n",
        "\n",
        "The second method is the **Min-Max Feature Scaling**, also called normalization, which scales the feature between 0 and 1. It's computed by subtracting from the input the minimum value in the column and subsequently dividing by the difference between the maximum and minimum value.\n",
        "\n",
        "The third method is the **Standard Scaler** and scales the data into a distribution with zero mean and variance 1.\n",
        "\n",
        "The last method is the **Robust Scaling** which removes the median and scales the data according to the quantile range.\n",
        "\n",
        "I've used StandardScaler."
      ],
      "metadata": {
        "id": "-cPkBaBkvcX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"MAS\": Maximum Absolute Scaling, \"MMS\": Min-Max Feature Scaling, \"SS\": Standard Scaler, \"Robust Scaling\"  \n",
        "s = \"SS\"\n",
        "def scaling(series, scaling_type):\n",
        "  if scaling_type == \"MAS\":\n",
        "    scaler = MaxAbsScaler()\n",
        "  elif scaling_type == \"MMS\":\n",
        "    scaler = MinMaxScaler()\n",
        "  elif scaling_type == \"SS\":\n",
        "    scaler = StandardScaler()\n",
        "  elif scaling_type == \"RS\":\n",
        "    scaler = RobustScaler()\n",
        "  else:\n",
        "    return series\n",
        "  scaler.fit(series)\n",
        "  scaled = scaler.fit_transform(series)\n",
        "  series = pd.DataFrame(scaled, columns = series.columns)\n",
        "  return series"
      ],
      "metadata": {
        "id": "QRS4QMiqu52L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the data"
      ],
      "metadata": {
        "id": "_t3_CWqIv0lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all the features\n",
        "def load_all(dataframe):\n",
        "  print('Loaded all the features and min_CPA of %d samples into X and y.' %(len(dataframe)))\n",
        "  #dividing input and targets\n",
        "  X = dataframe.iloc[:, :-2] \n",
        "  y = dataframe.iloc[:, -1]\n",
        "  # feature scaling\n",
        "  X = scaling(X, s)\n",
        "  return X, y\n",
        "\n",
        "\n",
        "# extract just one feature\n",
        "def load_one(feature_number, dataframe):\n",
        "  print('Loaded feature number %d and min_CPA of %d samples into X1 and y1.' %(feature_number, len(dataframe)))\n",
        "  #extracting the dataset with the required feature\n",
        "  dataframe1 = dataframe.loc[:, [list(dataframe.columns)[feature_number], 'min_CPA']]\n",
        "  # dividing input and targets\n",
        "  X1 = dataframe1.iloc[:, 0]\n",
        "  y1 = dataframe1.iloc[:, 1] \n",
        "  return dataframe1, X1, y1"
      ],
      "metadata": {
        "id": "lKkQuJNblZKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the file from drive and reading it into DataFrame\n",
        "filename = '/content/drive/MyDrive/Project_ML/Data/train_set.tsv'\n",
        "df = pd.read_csv(filename, sep = \"\\t\", header = 0)\n",
        "\n",
        "# load the full dataset\n",
        "X, y = load_all(df)\n",
        "\n",
        "# load just the feature specified in feature_number and the targets\n",
        "feature_number = 0\n",
        "df1, X1, y1 = load_one(feature_number, df)"
      ],
      "metadata": {
        "id": "x4WMV6XSzUmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlations and missing values"
      ],
      "metadata": {
        "id": "BnxMia89xtds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to know if there are some missing values in the dataset and eventually replace them. In our case, there aren't missing values. "
      ],
      "metadata": {
        "id": "zI8vbzbD1di7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of null cells in df: %d\" %(df.isnull().sum().sum()))\n",
        "print(\"Number of null cells in df1: %d\" %(df1.isnull().sum().sum()))"
      ],
      "metadata": {
        "id": "LaB8lBgp1VrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can even plot correlations between the features in the dataset. From there you can see that there are almost no correlations."
      ],
      "metadata": {
        "id": "LsrOCuLCWant"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.corr()\n",
        "df1.corr()"
      ],
      "metadata": {
        "id": "_PyWIg-5V4LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **DATA VISUALIZATION**\n",
        "\n"
      ],
      "metadata": {
        "id": "7yMXQcErjtjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regplot\n",
        "Here, i visualize the relation between two columns in df1. You can just choose the feature you want by taking x as 'df.iloc[:, feature_number]' and y as df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "s6UbuOx72V6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(x = df1.iloc[:, 0], y = df1.iloc[:, 1], fit_reg=False)"
      ],
      "metadata": {
        "id": "Eb8MfvMHjgo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "zUojl4nq8CAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **DATA SPLITTING**"
      ],
      "metadata": {
        "id": "NbOVBIQgwwE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting the dataset**\n",
        "I've splitted the dataset in training and test set.The model must be trained on the training data and then tested. Comparing predictions to targets in the test set can be seen as the unbiased performance evaluation of the model."
      ],
      "metadata": {
        "id": "B4RS7nrA4BJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "          test_size=0.3, random_state=24)"
      ],
      "metadata": {
        "id": "hBAPyapy3IKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **FIRST MODELS**\n",
        "Here, i've defined a bunch of models. I've decided to compare **SVR**, **DecisionTreeRegressor** and **RandomForestRegressor**. "
      ],
      "metadata": {
        "id": "TDHCmITk6-ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "rand_forest = RandomForestRegressor()\n",
        "svr = SVR()\n",
        "dtr = DecisionTreeRegressor()\n",
        "\n",
        "# list of models for evaluation purposes\n",
        "models = [rand_forest, svr, dtr]"
      ],
      "metadata": {
        "id": "KpDdnLqHYGGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**MODELS EVALUATION**\n",
        "Then, i've analyzed the main metrics for each model (defined previously) trained with the training set. I've decided to use mean squared error and r2 score. Thus, i've created a function that does exactly this, displaying the performances of the models on train and test set in a compact way."
      ],
      "metadata": {
        "id": "Rjs2wYuabJcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def models_scores(models, X_train, y_train, X_test, y_test): \n",
        "  # metrics lists for train\n",
        "  r2_train_list = []\n",
        "  mean_squared_train_list = []\n",
        "  \n",
        "  # metrics lists for test\n",
        "  r2_test_list = []\n",
        "  mean_squared_test_list = []\n",
        "  \n",
        "  names = []\n",
        "\n",
        "  for model in models:\n",
        "      #append the name of the model to the names list\n",
        "      names.append(type(model).__name__)\n",
        "\n",
        "      # fit the model and predict\n",
        "      model.fit(X_train,y_train)\n",
        "      y_pred_train = model.predict(X_train)\n",
        "      y_pred_test = model.predict(X_test)\n",
        "      \n",
        "\n",
        "      # compute the metrics for training set\n",
        "      mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "      r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "      # compute the metrics for test set\n",
        "      mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "      r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "      # add train metrics to the list \n",
        "      r2_train_list.append(r2_train)\n",
        "      mean_squared_train_list.append(mse_train)\n",
        "\n",
        "      # add test metrics to the list\n",
        "      r2_test_list.append(r2_test)\n",
        "      mean_squared_test_list.append(mse_test)\n",
        "      \n",
        "  d = {\n",
        "      'Model': names, \n",
        "      'R2_train': r2_train_list,\n",
        "      'R2_test': r2_test_list, \n",
        "      'MSE_train': mean_squared_train_list, \n",
        "      'MSE_test': mean_squared_test_list}\n",
        "  scores = pd.DataFrame(d)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "mQWBH6vK4ViD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_scores(models, X_train,y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "Z0MRRfWLa7rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **MODELS OPTIMIZATION**\n",
        "As you can see, the performances of models with default parameters were bad. Here, i've tried to improve the performances\n"
      ],
      "metadata": {
        "id": "SLgqfZ1De1Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# polynomial svr\n",
        "poly_svr = SVR(kernel='poly',C= 1, degree=6,  gamma=0.2)\n",
        "\n",
        "# decision tree regressor\n",
        "dt = DecisionTreeRegressor(criterion='poisson', \n",
        "                           splitter='best', \n",
        "                           max_depth=None, \n",
        "                           max_leaf_nodes=2)\n",
        "# random forest regressor\n",
        "ran_forest = RandomForestRegressor(n_estimators = 90, max_depth =3, criterion ='poisson')\n",
        "\n",
        "best_models = [poly_svr, dt, ran_forest]"
      ],
      "metadata": {
        "id": "9bxsZ8yzkkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Metrics**\n",
        "Below, the metrics of the new optimized models. The performances are slightly better. With SVR with polynomial kernel, i've got still a bad r2 score but visibly better than the others. "
      ],
      "metadata": {
        "id": "GKGrfW-phhhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_scores(best_models, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "g8FojT3gfRAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **HYPERPARAMETER TUNING**\n",
        "Since now, i've tuned the parameters by hand. Here, i've tried to improve performances using GridSearch. I used as scoring r2, which i thought to be one of the most representative performance metrics for this task.\n",
        "\n"
      ],
      "metadata": {
        "id": "lxYr6Dtulah_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grid search**\n",
        "\n",
        "I've searched for the model optimal combination of parameters, given multiple possible values. These values were obtained after reading the documentation of the regressors and after several trials. "
      ],
      "metadata": {
        "id": "aMNSuPQNE-RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your regressor: \"SVR\": Support Vector Machine Classifier, \"RFR\": Random Forest Regressor, \"DTR\": Decition Tree Regressor\n",
        "regr = 'DTR'\n",
        "def define_grid(regressor):\n",
        "  if regressor == 'SVR':\n",
        "    estimator  = SVR()\n",
        "    param_grid = {\n",
        "        'kernel': ['rbf', 'sigmoid', 'poly'],\n",
        "        'degree': np.arange(2, 10, 1),\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'gamma':[0.001, 0.01, 0.1, 0.014, 1, 10, 100]\n",
        "      }\n",
        "  elif regressor == 'RFR':\n",
        "    estimator = RandomForestRegressor()\n",
        "    param_grid = {\n",
        "        'n_estimators':[1, 5, 10, 15, 20, 50, 100, 200, 500, 1000],\n",
        "        'max_depth': np.arange(2, 7, 1),\n",
        "        'criterion': ['poisson', 'squared_error', 'absolute_error'],\n",
        "      }\n",
        "\n",
        "  elif regressor == 'DTR':\n",
        "    estimator = DecisionTreeRegressor()\n",
        "    param_grid ={\n",
        "        'criterion':['poisson', 'squared_error', 'absolute_error'],\n",
        "        'splitter':['best', 'random'], \n",
        "        'max_depth':[None, 1, 2, 3, 4, 5, 6, 8, 9, 10]\n",
        "  }\n",
        "  return estimator, param_grid\n"
      ],
      "metadata": {
        "id": "mVz-Iz1RdW-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator, param_grid = define_grid(regr)\n",
        "# grid search using r2\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid = param_grid, cv=3, n_jobs =-1, scoring='r2')"
      ],
      "metadata": {
        "id": "45u2DewPaBgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to avoid GridSearch skip this part."
      ],
      "metadata": {
        "id": "gGuGPO1HAi3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell only if you want to perform Grid Search. It will take some time.\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RCqE7aAqudWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd performed GridSearch, you can visualize the best parameters found for the chosen model and the dataset"
      ],
      "metadata": {
        "id": "CTGtEFMa_nQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best parameters for the classifier\n",
        "print(\"Best regression hyper-parameters for the chosen regressor: %r\" %grid_search.best_params_)\n",
        "print(\"Best r2: %.4f\" %grid_search.best_score_)"
      ],
      "metadata": {
        "id": "FDNXN8Nt35My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **BAGGING**\n",
        "In this section, i've tried bagging. Bagging is an ensemble learning technique where each estimator receives a random subset of examples from the training dataset. Once the individual estimators are fit to the bootstrap samples, the predictions are combined together.\n",
        "I've used BaggingRegressor from the scikit-learn library."
      ],
      "metadata": {
        "id": "iU2D28Gm98Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_estimator = SVR(kernel='poly',C= 500, degree=2,  gamma=0.075)\n",
        "bagging = BaggingRegressor(base_estimator=base_estimator, n_estimators=21, random_state=0)"
      ],
      "metadata": {
        "id": "oUeBrTLe-igM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalution\n",
        "As you can see, bagging performs nicely. Still, the performances are not very good. Surely, it reduces overfitting. "
      ],
      "metadata": {
        "id": "Gmw4iyIeeWxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = [bagging]\n",
        "models_scores(model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "9fxPlBzI-1bF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}